---
title: 神经网络基础概念总结
date: 2020-08-07 17:48:59
tags:
	- 机器学习
categories:
	- AI
	- 机器学习
fileName: nn-basic-concept-summary
---

### 神经网络复杂度

**NN复杂度：多用NN层数和NN参数的个数表示**

层数 = 隐藏层的层数+1个输出层

总参数 = 总w + 总b

**空间复杂度**：即为总参数个数

**时间复杂度**：即为乘加运算次数

![](http://cdn.ziyedy.top/image/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E6%80%BB%E7%BB%93/%E5%A4%8D%E6%9D%82%E5%BA%A6.png)



### 学习率

![](http://cdn.ziyedy.top/image/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E6%80%BB%E7%BB%93/%E5%AD%A6%E4%B9%A0%E7%8E%87.png)

#### 指数衰减学习率

先用较大的学习率，快速得到较优解，然后逐步减小学习率，使模型在训练后期稳定。公式如下：

```
指数衰减学习率 = 初始学习率 * 学习率衰减率 (当前轮数 / 多少轮衰减一次)
```



### 激活函数

![](http://cdn.ziyedy.top/image/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E6%80%BB%E7%BB%93/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.png)

#### 优秀的激活函数

- 非线性：激活函数非线性时，多层神经网络可逼近所有函数
- 可微性： 优化器大多用梯度下降更新参数
- 单调性： 当激活函数是单调的，能保证单层网络的损失函数是凸函数
- 近似恒等性： f(x)≈x 当参数初始化为随机小值时，神经网络更稳定



#### Sigmoid函数

$$
f(x) = \frac {1} {1 + e^{-x}}
$$

![](http://cdn.ziyedy.top/image/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E6%80%BB%E7%BB%93/sigmoid.png)

特点：

（1）易造成梯度消失
（2）输出非0均值，收敛慢
（3）幂运算复杂，训练时间长

#### Tanh函数

![](http://cdn.ziyedy.top/image/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E6%80%BB%E7%BB%93/tanh.png)

特点
（1）输出是0均值
（2）易造成梯度消失
（3）幂运算复杂，训练时间长

#### Relu函数（用的最多）

$$
f(x) = max(x, 0)
$$

![](http://cdn.ziyedy.top/image/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E6%80%BB%E7%BB%93/relu.png)

优点：

（1）解决了梯度消失问题 解决了梯度消失问题 (在正区间）

（2）只需判断输入是否大于 只需判断输入是否大于0，计算速度快

（3）收敛速度远快于 收敛速度远快于sigmoid 和tanh

缺点：

（1）输出非0均值，收敛慢

（2）Dead Relu问题：某些神经元永远不会被激活，导致相应的参数永远不会被更新

#### Leaky Relu函数

![](http://cdn.ziyedy.top/image/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E6%80%BB%E7%BB%93/Leaky-relu.png)

#### 激活函数的使用

1、首选relu激活函数

2、学习率设置较小值

3、输入特征标准化，即让输入特征满足以0为均值，1为标准差的正态分布； 

4、初始参数中心化，即让随机生成的参数满足以0为均值，$\frac {2} {当前层特征个数}$ 为标准差的正态分布



### 损失函数

神经网络的优化目标就是使得损失函数最小

#### 自定义损失函数

可以根据具体的项目自定义损失函数

#### 均方误差（MSE）

很好理解，平时最基本的曲线拟合就是用的均方误差的损失函数。
$$
MSE(y\_, y) = \frac {\sum^n_{i=1} (y-y\_)^2} {n}
$$


#### 交叉熵损失函数（Cross Entropy）

$$
H(y\_, y) = - \sum y\_ * ln y
$$

用来表征两个概率分布之间的距离，常与softmax函数结合

**输出先进行softmax函数，再计算y与y\_的交叉熵损失函数**

```
tf.nn.softmax_cross_entropy_with_logits(y_ ，y)
```





### 正则化

正则化一般用于**缓解过拟合（训练准确性和测试准确性之间的差距代表*过度拟合* ）**，即在损失函数中引入模型复杂度指标，利用给W加权值，弱化了训练数据的噪声

![](http://cdn.ziyedy.top/image/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E6%80%BB%E7%BB%93/%E6%AD%A3%E5%88%99%E5%8C%96.png)

#### L1正则化

$$
loss_{L1}(w) = \sum_{i} |x|
$$

L1正则化大概率会使很多参数变为0，因此该方法可通过稀疏参数，即减少参数的数量降低复杂度

#### L2正则化

$$
loss_{L1}(w) = \sum_{i} |x^2|
$$

L2正则化会使参数很接近零但不为零，因此该方法可通过减小参数值的大小降低复杂度。





### 参数优化器

#### SGD



#### Adam



