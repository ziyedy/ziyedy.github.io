---
title: 机器学习基础方法与应用
date: 2020-10-12 12:45:42
tags:
	- 机器学习
	- sklearn
categories:
	- AI相关
	- 机器学习
fileName: sklearn-ml
---

## 分类：K近邻算法（KNN）

### sklearn使用

```
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=6)
knn.fit(x_train, y_train)	# 进行训练
knn.score(x_test, y_test)	# 获取训练得分
y_predict = knn.predict(x_predict)
```



### 决策边界





## 回归：线性回归

### 基本思想



### sklearn实现



```
from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(X_train, Y_train)
lin_reg.coef_   # 系数矩阵
lin_reg.intercept_  # 截距
```



多项式回归使用线性回归的思路，为原来的样本添加新的特征，原有的特征的多项式组合





## 多项式回归

### 基本思想



### sklearn使用多项式回归

```
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2)  # 指定x的最高维度为2
poly.fit(x)
x2 = poly.transform(x)  # 输出x^0 x^1 x^2作为输入线性回归的参数
```



### pipeline

```
from sklearn.pipeline import Pipeline

poly_reg = Pipeline([
    ("Poly", PolynomialFeatures(degree=3)),
    ("std_scaler", StandardScaler()),
    ("lin_reg", LinearRegression())
])
poly_reg.fit(x, y)
poly_reg.predict(x)
```







## sklearn基本操作

### 数据集的获取与创建

```
from sklearn import datasets
from sklearn.model_selection import train_test_split

# 获取包中自带的数据集，以鸢尾花数据集为例
iris = datasets.load_iris()
x = iris.data
y = iris.target

# 将数据集分割为测试数据集和训练数据集
# 可以传入测试数据集的比例以及一个随机种子
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=666)
```



```
from sklearn.metrics import accuracy_score

accuracy_score(y_test, y_predict)
```



### 超参数搜索

> 用于搜索确定获取最优解的超参数
>
> GridSearchCV中的CV指的是交叉验证

```
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier

# 定义参数的搜索范围
param_grid = [
    {
        'weights': ['uniform'],
        'n_neighbors': [i for i in range(1, 4)]
    },
    {
        'weights': ['distance'],
        'n_neighbors': [i for i in range(1, 4)],
        'p': [i for i in range(1, 6)]
    }
]

knn_clf = KNeighborsClassifier()

# 进行网格搜索，n_jobs代表使用核的个数
grid_search = GridSearchCV(knn_clf, param_grid, n_jobs=-1)
grid_search.fit(x_train, y_train)

grid_search.best_estimator_ # 获取最佳分类器
grid_search.best_score_     # 获取最佳参数的准确度
grid_search.best_params_    # 获取最佳参数
knn_clf = grid_search.best_estimator_   # 将最佳分类器赋给原模型

```



### 归一化处理

> 即将所有数据映射到同一尺度，包括最值归一化（映射到0-1之间）与均值方差归一化（将所有数据归一到均值为0方差为1的分布中）
>
> 对测试数据集进行归一化处理时，通常使用训练数据集的均值与方差等统计数据

scikit-learn中使用Scaler，实现*均值方差归一化*

```
from sklearn.preprocessing import StandardScaler

standardScaler = StandardScaler()
standardScaler.fit(x_train)
standardScaler.mean_    # 不是用户传来的变量，而是用户传进去的数据计算出来的变量
standardScaler.scale_   # 得到标准差
x_train = standardScaler.transform(x_train)
x_test = standardScaler.transform(x_test)
```

仿照`sklearn`中的对应类实现一个均值方差归一化的类。

```
import numpy as np

class StandardScaler:

    def __init__(self):
        self.mean_ = None
        self.scale_ = None

    def fit(self, X):
        """ 计算X获得数据的均值与标准差 """
        self.mean_ = np.array([np.mean(X[:, i]) for i in range(X.shape[1])])
        self.scale_ = np.array([np.std(X[:, i]) for i in range(X.shape[1])])
        return self

    def transform(self, X):
        """ 将X进行均值方差归一化处理 """
        res = np.empty(shape=X.shape, dtype=float)
        for col in range(X.shape[1]):
            res[:, col] = (X[:, col] - self.mean_[col]) / self.scale_[col]
        return res
```



### 模型正则化

导致偏差：对问题本身的定义不正确（选取不相关的特征），欠拟合（线性回归天生高偏差，对数据有极强的假设）

导致方差：数据的扰动会影响，模型太复杂（KNN天生高方差，对数据依赖）

大多数算法具有相应的参数，可以调整偏差与方差



解决高方差：降低模型复杂度、减少数据噪声、增加样本数、实验验证集、模型正则化



模型正则化：限制参数的大小







